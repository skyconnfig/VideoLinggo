import os
import json
import time
import requests
import functools
from threading import Lock
import json_repair
from openai import OpenAI
from core.utils.config_utils import load_key
from rich import print as rprint
from core.utils.decorator import except_handler

# ------------
# cache gpt response
# ------------

LOCK = Lock()
GPT_LOG_FOLDER = 'output/gpt_log'

def _save_cache(model, prompt, resp_content, resp_type, resp, message=None, log_title="default"):
    with LOCK:
        logs = []
        file = os.path.join(GPT_LOG_FOLDER, f"{log_title}.json")
        os.makedirs(os.path.dirname(file), exist_ok=True)
        if os.path.exists(file):
            with open(file, 'r', encoding='utf-8') as f:
                logs = json.load(f)
        logs.append({"model": model, "prompt": prompt, "resp_content": resp_content, "resp_type": resp_type, "resp": resp, "message": message})
        with open(file, 'w', encoding='utf-8') as f:
            json.dump(logs, f, ensure_ascii=False, indent=4)

def _load_cache(prompt, resp_type, log_title):
    with LOCK:
        file = os.path.join(GPT_LOG_FOLDER, f"{log_title}.json")
        if os.path.exists(file):
            with open(file, 'r', encoding='utf-8') as f:
                for item in json.load(f):
                    if item["prompt"] == prompt and item["resp_type"] == resp_type:
                        return item["resp"]
        return False

# ------------
# ask gpt once
# ------------

def _check_ollama_service(base_url):
    """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨"""
    try:
        # ç§»é™¤/v1åç¼€è¿›è¡Œå¥åº·æ£€æŸ¥
        health_url = base_url.replace('/v1', '').rstrip('/')
        response = requests.get(f"{health_url}/api/tags", timeout=5)
        return response.status_code == 200
    except Exception as e:
        rprint(f"[yellow]âš ï¸ Ollama service check failed: {e}[/yellow]")
        return False

def _enhanced_error_handler(error_msg, retry=5, delay=1):
    """å¢å¼ºçš„é”™è¯¯å¤„ç†è£…é¥°å™¨ï¼Œä¸“é—¨é’ˆå¯¹502é”™è¯¯ä¼˜åŒ–"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            base_url = load_key("api.base_url")
            
            for i in range(retry + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    error_str = str(e)
                    
                    # è¯¦ç»†çš„é”™è¯¯åˆ†ç±»å’Œæ—¥å¿—
                    if "502" in error_str or "Bad Gateway" in error_str:
                        rprint(f"[red]ğŸ”„ 502 Bad Gateway error (attempt {i+1}/{retry+1}): OllamaæœåŠ¡æš‚æ—¶ä¸å¯ç”¨[/red]")
                        # 502é”™è¯¯æ—¶æ£€æŸ¥æœåŠ¡çŠ¶æ€
                        if not _check_ollama_service(base_url):
                            rprint(f"[yellow]âš ï¸ OllamaæœåŠ¡è¿æ¥å¤±è´¥ï¼Œç­‰å¾…æœåŠ¡æ¢å¤...[/yellow]")
                    elif "timeout" in error_str.lower():
                        rprint(f"[red]â±ï¸ Timeout error (attempt {i+1}/{retry+1}): è¯·æ±‚è¶…æ—¶[/red]")
                    elif "connection" in error_str.lower():
                        rprint(f"[red]ğŸ”Œ Connection error (attempt {i+1}/{retry+1}): è¿æ¥å¤±è´¥[/red]")
                    else:
                        rprint(f"[red]{error_msg}: {e}, retry: {i+1}/{retry+1}[/red]")
                    
                    if i == retry:
                        rprint(f"[red]âŒ æœ€ç»ˆå¤±è´¥: {error_msg} - {e}[/red]")
                        raise last_exception
                    
                    # æŒ‡æ•°é€€é¿ï¼Œä½†å¯¹502é”™è¯¯ä½¿ç”¨æ›´é•¿çš„ç­‰å¾…æ—¶é—´
                    if "502" in error_str:
                        wait_time = delay * (3 ** i)  # 502é”™è¯¯ä½¿ç”¨3çš„æŒ‡æ•°
                    else:
                        wait_time = delay * (2 ** i)  # å…¶ä»–é”™è¯¯ä½¿ç”¨2çš„æŒ‡æ•°
                    
                    rprint(f"[cyan]â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...[/cyan]")
                    time.sleep(wait_time)
        
        return wrapper
    return decorator

def _direct_api_request(base_url, model, messages, response_format=None, timeout=60):
    """ç›´æ¥ä½¿ç”¨requestsè°ƒç”¨Ollama APIï¼Œç»•è¿‡OpenAIå®¢æˆ·ç«¯åº“å…¼å®¹æ€§é—®é¢˜"""
    api_url = f"{base_url.rstrip('/')}/chat/completions"
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {load_key('api.key')}"
    }
    
    payload = {
        "model": model,
        "messages": messages,
        "stream": False
    }
    
    if response_format:
        payload["response_format"] = response_format
    
    rprint(f"[cyan]ğŸ”§ ä½¿ç”¨ç›´æ¥APIè¯·æ±‚: {api_url}[/cyan]")
    
    response = requests.post(
        api_url,
        headers=headers,
        json=payload,
        timeout=timeout
    )
    
    if response.status_code != 200:
        raise Exception(f"API request failed with status {response.status_code}: {response.text}")
    
    return response.json()

@_enhanced_error_handler("GPT request failed", retry=5, delay=2)
def ask_gpt(prompt, resp_type=None, valid_def=None, log_title="default"):
    if not load_key("api.key"):
        raise ValueError("API key is not set")
    # check cache
    cached = _load_cache(prompt, resp_type, log_title)
    if cached:
        rprint("use cache response")
        return cached

    model = load_key("api.model")
    base_url = load_key("api.base_url")
    if 'ark' in base_url:
        base_url = "https://ark.cn-beijing.volces.com/api/v3" # huoshan base url
    elif 'v1' not in base_url:
        base_url = base_url.strip('/') + '/v1'
    
    # åœ¨è¯·æ±‚å‰æ£€æŸ¥æœåŠ¡çŠ¶æ€
    if "localhost" in base_url and not _check_ollama_service(base_url):
        rprint(f"[yellow]âš ï¸ OllamaæœåŠ¡ä¼¼ä¹æœªå“åº”ï¼Œä½†ç»§ç»­å°è¯•è¿æ¥...[/yellow]")
    
    response_format = {"type": "json_object"} if resp_type == "json" and load_key("api.llm_support_json") else None
    messages = [{"role": "user", "content": prompt}]
    
    # ä¼˜åŒ–è¶…æ—¶è®¾ç½®ï¼šæœ¬åœ°æœåŠ¡ä½¿ç”¨è¾ƒçŸ­è¶…æ—¶ï¼Œäº‘æœåŠ¡ä½¿ç”¨è¾ƒé•¿è¶…æ—¶
    timeout = 60 if "localhost" in base_url else 120
    
    rprint(f"[cyan]ğŸ¤– æ­£åœ¨è¯·æ±‚ {model} æ¨¡å‹...[/cyan]")
    
    # å¯¹äºæœ¬åœ°OllamaæœåŠ¡ï¼Œä½¿ç”¨ç›´æ¥APIè¯·æ±‚é¿å…OpenAIå®¢æˆ·ç«¯åº“å…¼å®¹æ€§é—®é¢˜
    if "localhost" in base_url:
        try:
            resp_raw = _direct_api_request(base_url, model, messages, response_format, timeout)
            resp_content = resp_raw['choices'][0]['message']['content']
        except Exception as e:
            rprint(f"[yellow]âš ï¸ ç›´æ¥APIè¯·æ±‚å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨OpenAIå®¢æˆ·ç«¯åº“: {e}[/yellow]")
            # å›é€€åˆ°OpenAIå®¢æˆ·ç«¯åº“
            client = OpenAI(api_key=load_key("api.key"), base_url=base_url)
            params = dict(
                model=model,
                messages=messages,
                response_format=response_format,
                timeout=timeout
            )
            resp_raw = client.chat.completions.create(**params)
            resp_content = resp_raw.choices[0].message.content
    else:
        # å¯¹äºäº‘æœåŠ¡ï¼Œç»§ç»­ä½¿ç”¨OpenAIå®¢æˆ·ç«¯åº“
        client = OpenAI(api_key=load_key("api.key"), base_url=base_url)
        params = dict(
            model=model,
            messages=messages,
            response_format=response_format,
            timeout=timeout
        )
        resp_raw = client.chat.completions.create(**params)
        resp_content = resp_raw.choices[0].message.content

    # process and return full result
    if resp_type == "json":
        resp = json_repair.loads(resp_content)
    else:
        resp = resp_content
    
    # check if the response format is valid
    if valid_def:
        valid_resp = valid_def(resp)
        if valid_resp['status'] != 'success':
            _save_cache(model, prompt, resp_content, resp_type, resp, log_title="error", message=valid_resp['message'])
            raise ValueError(f"â API response error: {valid_resp['message']}")

    _save_cache(model, prompt, resp_content, resp_type, resp, log_title=log_title)
    rprint(f"[green]âœ… GPTè¯·æ±‚æˆåŠŸå®Œæˆ[/green]")
    return resp


if __name__ == '__main__':
    from rich import print as rprint
    
    result = ask_gpt("""test respond ```json\n{\"code\": 200, \"message\": \"success\"}\n```""", resp_type="json")
    rprint(f"Test json output result: {result}")
